---
title: "Causal Inference for Data Science"
subtitle: "ITAM Short Workshop"
author: "Mathew Kiang, Zhe Zhang, Monica Alexander"
date: "March 15, 2017"
output:
  xaringan::moon_reader:
    css: ["custom.css", "./../custom.css", ]
    # In order for the css file to work, you need to set your working directory
    # to one above the slide directory ('./../') and then call moon_reader via
    # `xaringan::inf_mr('./part-01-intro/index.Rmd')`
    # Or just knit it into a browser and it should work immediately.
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
name: inverse
layout: true
class: center, middle, inverse

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

---

# Statistics of Causal Inference

- linear regression; with causal interpretation
- possible causal biases / "mis-identification"
- linear regression is not restrictive

???
We're going to cover these three topics.
First, Matt introduced the importance of casual identification and causal questions.
He also emphasized the Rubin Causal Model (potential outcomes framework).
We'll return to this throughout the next few lectures.
In this lecture, we talk about the basic statistical tools we use to identify causal estimates. Specifically in much of the literature, it is linear regression.

In ML and data science, usually we like linear regression for a first attempt, but it's not really a "hot" model to use in general. This is because it usually is not the best predictor. However, in this case, the best prediction is not what we care about.

Instead, we want to be able to identify a causal estimate. And an unbiased causal estimate.

Linear regression is interpretable. We would like better prediction, but that's usually not the first priority. Let's take this next image as an example.

---
# Job Earnings: Predicting vs Causal Estimate

![CEF_example](./assets/mhe_cef.png)
??
In this example, we have years of education on the x-axis and wage earnings on the y-axis.
The grey areas represent the distribution of data from each person with that wage.
We're clearly not going to try to explain such a variable thing. 
Instead, we start with the simple idea that there is clear causal pattern due to years of education.

---

# Review of Linear Regression

- (continuous) outcome is additive in the features
$$Y_i = \beta_0 + \beta \mathbf{X_i} + \epsilon_i$$
--
$$Y_i = \beta_0 + \gamma T_i + \beta \mathbf{X_i} + \epsilon_i$$

- not limited to simple linear possibilities (interactions, feature transformation, semi-parametric methods)

??
- Going to spend a lot of time thinking about $\gamma$ and $\epsilon_i$.
- Though we will just use regression, this does not limit our scope much:

---
# Causal Linear Regression

- What we really care about:
$$ E[Y|T=T_i] $$

![CEF_regression](./assets/mhe_cef_regression.png)

???
We really care about changes in this value. How does the expected value of Y (outcome) change?

---
# Recreate Graphic

```{r, message=F, error=FALSE, warning=F}
library(tidyverse); library(haven)
df <- read_dta('../datasets/ak_91_iv_qob.dta') %>%
  mutate(weekly_wage = exp(lnw))
year_df <- df %>% group_by(s) %>%
  summarise(mean_lw = mean(lnw))
g_complex <- ggplot(year_df, aes(x = s, y = mean_lw)) + 
  geom_point() + 
  geom_smooth(span = 0.35, se = F, color = 'red') +
  ggtitle("CEF with complex fit")
g_simple <- ggplot(year_df, aes(x = s, y = mean_lw)) + 
  geom_point() + 
  geom_smooth(method = 'lm', se = F, color = 'red') +
  ggtitle("CEF with simple linear regression")
```

```{r}
g_simple
```

---

# Causal Regression Interpretation

- estimating systematic randomness; 
  - we know we can't explain (close to) everything
  - instead, we want *unbiased* estimates of particular causal patterns
- argue that the conditional expectation function (CEF) is causal

??
We know that it's not that simple to explain complex real-world outcomes with a linear model.
We need to argue using using theory that our estimate of the CEF is okay.

However, if we believe we have an unbiased estimate, this is useful approximation. The exact numbers usually aren't that crucial. 
And linear models are fairly robust.

---
# CEF

$$Y = E[Y|T] + \epsilon$$

???
This is the best MSE predictor, no matter. What we need to do is to find a functional form of the explained part.
By definition, Epsilon is uncorrelated and mean-zero. 

---

# Best Linear Approximation to CEF

- True generative process: $Y = E[Y|T]+\epsilon \approx \gamma T + \epsilon$.
- $E[\epsilon] = 0$
  - how does the distribution of $Y_i$ change *wrt* $X_i$?
  - linear regression gives us interpretable coefficients
  - Only the average causal effect
  
- True $\gamma, \epsilon$ are unknown. Instead, we estimate using observations $i$: 
$$Y_i = \hat_{\gamma} T_i + \hat{\epsilon_i}$$
  
???
Even if truly non-linear, there is some use to have a robust linear approximation.


---
# Linear Regression With Binary Treatment

$$E[Y_i|T=T_1] - E[Y_i|T=T_0] = \\ \gamma (T_1 - T_0) + \\ E[\epsilon_i|T_1] - E[\epsilon_i|T_0]$$

$$E[Y_i|T=T_1] - E[Y_i|T=T_0] = \gamma + (0 - 0)$$

---
# Linear Regression More Generally

$$E[Y_i|T=T_a] - E[Y_i|T=T_b] = \\ \gamma (T_a - T_b) + \\ E[\epsilon_i|T_a] - E[\epsilon_i|T_b]$$

$$E[Y_i|T=T_a] - E[Y_i|T=T_b] = \gamma (T_a - T_b)$$

---
# (Causal) Linear Regression Gone Wrong

$$E[Y_i|T=T_a] - E[Y_i|T=T_b] = \\ \gamma (T_a - T_b) + \\ E[\epsilon_i|T_a] - E[\epsilon_i|T_b]$$

What if?:

$$E[\epsilon_i|T_a] >> E[\epsilon_i|T_b]$$

Then:

$$E[Y_i|T=T_a] - E[Y_i|T=T_b] = \gamma (T_a - T_b) + Bias_{OVB}$$

---
# Example of OVB

# (Causal) Linear Regression Gone Wrong

$$E[Y_i|Loc_A] - E[Y_i|Loc_B] = \\ \gamma + \\ E[\epsilon_i|Loc_A] - E[\epsilon_i|Loc_B]$$

$\epsilon_i$ could be higher in $Loc_A$ because of the wealthy parents and peers effect, while $\epsilon_i$ could be lower in $Loc_B$ because of nearby crime and travel time.

- Occurs because assumed linear model is wrong. The true model may be $Y = \gamma Loc_A + \beta wealth_{parents} + \epsilon$.

---
# Omitted Variable Bias Confounding

- true correlation between {$T_i$, $\epsilon$} (not $\hat{\epsilon}$) causes problems
- AKA: **omitted variable bias**, **confounding**

---

# Note on Using Causal Terminology

- causal: {"effect", "leads to", "results in", "because of", "impact"}
- observed patterns: {"related with", "pattern", "correlation", "tends to move with", "observed"}

---

# Conditional Independence ("controls")

- getting a Master's degree may not be appropriately random
- people who are prescribed a drug treatment may not be comparable to general population

- it could be argued though:
  - students with similar GPA, age, and Bachelor degrees may be random
  - people with similar blood measurements seen by different doctors may have random drug treatment
  
- ${Y_{0i}, Y_{1i}} \bot T_i ?$

???
Previously we assumed that 

--

- ${Y_{0i}, Y_{1i}} \bot T_i | \mathbf{X_i}?$
- $Y_{ti} \bot T_i | \mathbf{X_i}\quad\forall t?$

---

# Avg Causal Effect

$$
\begin{align*}
\ & E[Y_{i}|X_{i},s_{i}=s]-E[Y_{i}|X_{i},s_{i}=s-1] \\
\ & =E[Y_{si}-Y_{(s-1)i}|X_{i}]
\end{align*}
$$

--
- but there's an $X_i$ there?

# Linear Regression with CIA

- linear additive assumption simplifies things

$$
\begin{align*}
\ \gamma_{s_i} & =E[Y_{i}|X_{i},s_{i}=s]-E[Y_{i}|X_{i},s_{i}=s-1]\\
 & =E[Y_{si}-Y_{(s-1)i}] + \beta X_i - \beta X_i\\
 & =E[Y_{si}-Y_{(s-1)i}]
\end{align*}
$$

---

# Possibles Biases (many!)

observational data

(almost) All data is observational.

If you think you have an experiment, it's probably still observational data.

- omitted variable bias
- selection bias
- regression form bias

??
OVB and selection bias are similar

---
# Omitted Variable Bias

when Conditional Independece is violated

- omitted variables = (uncontrolled) correlation with the $T_i$ 
- leads the error term $\epsilon_i$ to affect estimate of $\gamma$
- **confounding**

???
- most common occurrence in analysis, i.e. OVB
- up to analyst to decide if OVB affects insights
- occurs when omitted variable affects both the outcome and a predictor

---
# OVB Illustrated

[todo] replace this

![illustrated_confound](./assets/iv_confound.png)

---
# OVB Examples

![ovb_education](./assets/ovb_estimates.png)

---
# OVB Examples

```{r}
library(tidyverse)
df_educ <- data_frame(iq = runif(200, 1, 100),
                      school_years = 16 - 0.05*iq + rnorm(200),
                      income = 20000 + 5000*school_years + 5000*rnorm(200) + 500*iq)
summary(lm(income ~ school_years, data = df_educ))
summary(lm(income ~ school_years + iq, data = df_educ))
```


---
# What does it mean to "control for covariates"?

- makes observed outcomes independent of our causal treatment/action of interest
- based on theory only though; there can always be omitted variable bias
  - and bias caused by bad controls
  - and functional form biases

---
# Omitted Variables

- we rarely "prove" causality
- instead, we argue based on domain expertise and theory that we are not facing biases
- often, a good analysis adjusts a lot of the assumptions to ensure the results are "robust" to possible biases


---
# OVB Thought Examples

what observational data is available?

- effect of air quality policy
- effect of advertising campaign
- effect of OTC drug campaign
- effect of customer loyalty campaign
- adoption of a mobile app
- calling customer service


---
# Selection Bias

(usually, this falls under omitted variable bias)

- sometimes selection bias can be also important for external validity
- "average" causal effect may be poorly estimated 

---
# Regression Functional Form Bias

- non-linear outcome variables
  - wages is usually not linear in the outcomes
    - we usually use $\log (wage)$ instead!
    - $\log(wage) ~ \beta educ$, now $\beta$ is interpreted as a percent increase in wage
- sometimes our features are non-linear too $\beta_i age + \beta_j age^2$
- 0/1 outcome variable?
  - logistic regression is common
  - Probit regression has a more economic interpretation, based on an individual's response to 
  - Poisson regression for counts
- Truncated/Censored Observation (e.g., test scores, wage)
- Attrition bias

- *important to think critically about where the (potential) data comes from or would come from*
- *almost all interesting data comes from human decisions, which is complex and may have several things affecting it*

---
# Extensions to Linear Form

- interactions: $\beta_j X_j \cdot X_k$
- feature transformations: $log(Y_i)$
- flexible semi-parametric: $Y_i = \beta_0 + \gamma T_i + \beta f(\mathbf{X_i}) + \epsilon_i $

---
# Despite Caution, Causal Work Still Useful

- Raj Chetty, policy impact
- Work on piracy affecting movie studios
- Health insurance

---
# Sources

- Cameron & Triveti textbook
- Mostly Harmless Econometrics
- Osea Giuntella, slides