---
title: "Causal Inference for Data Science"
subtitle: "ITAM Short Workshop"
author: "Mathew Kiang, Zhe Zhang, Monica Alexander"
date: "March 15, 2017"
output:
  xaringan::moon_reader:
    css: ["custom.css", "./../custom.css", ]
    # In order for the css file to work, you need to set your working directory
    # to one above the slide directory ('./../') and then call moon_reader via
    # `xaringan::inf_mr('./part-01-intro/index.Rmd')`
    # Or just knit it into a browser and it should work immediately.
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
name: inverse
layout: true
class: center, middle, inverse

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

---

# Statistics of Causal Inference

- linear regression; with causal interpretation
- possible causal biases / "mis-identification"
- linear regression is not a restrictive methodology

???
We're going to cover these three topics.
First, Matt introduced the importance of casual identification and causal questions.
He also emphasized the Rubin Causal Model (potential outcomes framework).
We'll return to this throughout the next few lectures.
In this lecture, we talk about the basic statistical tools we use to identify causal estimates. Specifically in much of the literature, it is linear regression.

In ML and data science, usually we like linear regression for a first attempt, but it's not really a "hot" model to use in general. This is because it usually is not the best predictor. However, in this case, the best prediction is not what we care about.

Instead, we want to be able to identify a causal estimate. And an unbiased causal estimate.

Linear regression is interpretable. We would like better prediction, but that's usually not the first priority. Let's take this next image as an example.

---
# Statistical Basics

- $Y$ = Outcome of Interest
- $T$ = Treatment
- $E[Y|T]$ = Conditional Expectation Function

First, we discuss the CEF; second, when is the CEF causal?

---
# Predicting Job Earnings

![CEF_example](./assets/mhe_cef.png)

- What about all the extra noise in wages?
--

- What does the above line mean to us? Interpret?

???
In this example, we have years of education on the x-axis and wage earnings on the y-axis.
The grey areas represent the distribution of data from each person with that wage.
We're clearly not going to try to explain such a variable thing. 

Instead, we're looking at the systematic pattern between education and wages.

---
# Recreate Graphic

```{r, message=F, error=FALSE, warning=F}
library(tidyverse); library(haven)
df <- read_dta('../datasets/ak_91_iv_qob.dta') %>%
  mutate(weekly_wage = exp(lnw))
year_df <- df %>% group_by(s) %>%
  summarise(mean_lw = mean(lnw))
g_CEF <- ggplot(year_df, aes(x = s, y = mean_lw)) + 
  geom_point() + 
  geom_smooth(span = 0.35, se = F, color = 'blue', method = 'loess') +
  geom_smooth(method = 'lm', se = F, color = 'red') +
  ggtitle("CEF between education & log-wage (with both complex & simple fit)")
```

---
# Recreate Graphic

```{r}
g_CEF
```

---
# Linear Regression Estimates a Linear CEF

CEF generally: $Y = E[Y|T,X] + \epsilon$

- basic interpretation: assume $E[Y|T,X]$ is **linear**

$$\hat{Y_i} = [\hat{\beta}_0 + \hat{\gamma} T_i] + \hat{\epsilon_i}$$
--

$$\hat{Y_i} = [\hat{\beta}_0 + \hat{\gamma} T_i + \hat{\mathbf{\beta}} \mathbf{X_i}] + \hat{\epsilon_i}$$

*Note: linear form is not that restrictive*
*Note: not the only interpretation (weighted regression is an alternative)*

???
This is the best MSE predictor, no matter. What we need to do is to find a functional form of the explained part.
By definition, Epsilon is uncorrelated and mean-zero. 

- In the usual linear regression, we only interpret the beta coefficients as helping to use X to explain variation in Y.
Now, we'll include a causal effect of interest, gamma, and make sure that we're estimating gamma correctly. This will be the crux of what we're focusing on in this class.

- Going to spend a lot of time thinking about $\gamma$ and $\epsilon_i$.
- Though we will just use regression, this does not limit our scope much:
not limited to simple linear possibilities (interactions, feature transformation, semi-parametric methods)


---
# Causal Linear Regression, Causal CEF

So far, this applies for all linear regression.  
When can we argue that the CEF is *causal*?

*Does the CEF have (conditional) indepedence from the potential outcomes?*

--

Consider observations $Y_i, T_i$ where T_i is a binary treatment.

$$\{ Y_{0i}, Y_{1i} \}$$

$$Y_i = Y_{0i} + (Y_{1i}-Y_{0i})T_i$$

Simple linear regression: $Y_i \sim \gamma T_i + \epsilon_i$

$$avgEffect = \hat{\gamma} = E[Y_i|T_i=1] - E[Y|T_i=0]$$ 

---
# Causal Linear Regression, Causal CEF

Simple linear regression: $Y_i \sim \gamma T_i + \epsilon_i$

$$avgEffect = \hat{\gamma} = E[Y_i|T_i=1] - E[Y|T_i=0]$$ 

Is this causal?

--

$$ CIA \equiv \left\{ Y_{0i}, Y_{1i} \right\} \bot T_i$$

To see why:

$$avgEffect = \hat{\gamma} = E[Y_i|T_i=1] - E[Y|T_i=0]$$ 

$$\hat{\gamma} = E[Y_{1i} - Y_{0i} | T_i = 1] - \left\{ E[Y_{0i}|T_i=1] - E[Y_{0i}|T_i=0] \right\}$$

The second term must be 0.
*The second term is also what will most difficult to convince people of.*

*Note: average treatment effect*


???
This is the potential outcomes change that we want.
And this is the indepdence that we want.

---
# Conditional Independence Assumption (CIA)

Previous example works for an RCT.
- built-in conditional indepdence assumption (CIA)

In observation data, we usually need to manipulate data to get CIA
- "conditional on other covariates, $\mathbf{X_i}$, we get CIA"

$$\left\{ Y_{0i}, Y_{1i} \right\} \neg\bot T_i \qquad \left\{ Y_{0i}, Y_{1i} \right\} \bot T_i|\mathbf{X_i}$$

--

$$\hat{Y_i} = [\hat{\beta}_0 + \hat{\gamma} T_i + \hat{\mathbf{\beta}} \mathbf{X_i}] + \hat{\epsilon_i}$$

*e.g. conditioning on neighborhood wealth, whether you go to a 'good' school or a 'bad' school is independent of your counterfactuals*

???
Obvious to avoid that good people go to good schools.
Also, avoid that people who are bad go to good schools.

---
# What about non-binary treatments?

Linearity assumption makes things simpler.

![CEF_example](./assets/mhe_cef_regression.png)

$$CIA = \left\{ Y_{t,i}, Y_{(t-1),i} \right\} \bot t_i|\mathbf{X_i} \quad \forall t$$

Do we need to estimate a causal effect for each possible value of $\mathbf{X_i}$?

$$E[Y_i|T=T_a,X_i] - E[Y_i|T=T_b,X_i] = \gamma (T_a - T_b) + (\beta X_i - \beta X_i)$$

(a) No, we can focus on the overall average treatment effect.
(b) simple, robust interpretation: assumes that $E[Y|T]$ is linear.
- Then, in the linear regression, $\mathbf{X_i}$ controls for the other variation

In other notation, the linear assumption gives us:
$$\hat{\gamma} = \frac{\partial{Y}}{\partial{T}}$$

???
Important to be cognizant that this is what you're estimating however.

---

# Non-Math Causal Regression Discussion

Review:

(almost) All data is observational.
If you think you have an experiment, it's probably still observational data.

- "estimating systematic randomness"
  - we know we can't explain (close to) everything
  - instead, we want *unbiased* estimates of particular causal patterns
  - how does the distribution of $Y_i$ change *wrt* $X_i$?
- important to argue for conditional independence assumption (CIA)
  - we rarely "prove" causality
  - instead, we argue based on domain expertise and theory that we are not facing biases
  - often, a good analysis adjusts a lot of the assumptions to ensure the results are "robust" to possible biases
- (best) linear approximation to the true CEF
- regression gives us interpretable coefficients, for the **average** effect
- True $\gamma, \epsilon$ are unknown. Instead, we estimate using observations $i$.

???
We know that it's not that simple to explain complex real-world outcomes with a linear model.
We need to argue using using theory that our estimate of the CEF is okay.

However, if we believe we have an unbiased estimate, this is useful approximation. The exact numbers usually aren't that crucial. 
And linear models are fairly robust.

Even if truly non-linear, there is value in having a robust linear approximation.

---

# Note on Using Causal Terminology

- causal: {"effect", "leads to", "results in", "because of", "impact"}
- observed patterns: {"related with", "pattern", "correlation", "tends to move with", "observed"}

---
# Attacks on the CIA assumption

- omitted variable bias, selection bias
- external validity
- regression functional form
- measurement error
- reverse causality

---
# Omitted Variable Bias: concerns in $\epsilon$ term

Consider a treatment bringing people from $T_b$ to $T_a$.

$$CEF_{TRUE} \equiv Y = \gamma T + \beta \mathbf{X_i} + \delta Location + \nu_i$$

--

But we ignore location effects.

$$CEF_{estimate} \equiv Y_i = \hat{\gamma} T_i + \beta \mathbf{X_i} + \epsilon_i$$

What is the $E[\hat{\epsilon_i}]$? What is the $E[\epsilon_i]$ ?

--

$$E[\epsilon_i|T_i,X_i] \ne 0 \quad ?$$

In other words

$$\epsilon_{i} \bot{} \left\{ T_i, X_i \right\}$$

---
# Omitted Variable Bias: concerns in $\epsilon$ term

Consider a treatment bringing people from $T_b$ to $T_a$.

$$avgEffect = E[Y_i|T=T_a,X_i] - E[Y_i|T=T_b,X_i] = \\ \gamma (T_a - T_b) + \beta X_i - \beta X_i + \\ E[\epsilon_i|T_a] - E[\epsilon_i|T_b]$$

$$E[\epsilon_i|T_a] >> E[\epsilon_i|T_b]$$

Omitted variable is "hidden" inside $\epsilon_i$ between those treated and untreated.

$$E[Y_i|T=T_a] - E[Y_i|T=T_b] = \gamma (T_a - T_b) + Bias_{OVB}$$

$E[\epsilon_i|Loc_A]$ could be **positive** because of the wealthy parents and peers effect
$E[\epsilon_i|Loc_B]$ could be **negative** because of nearby crime and travel time.

---
# OVB Examples

![ovb_education](./assets/ovb_estimates.png)

---
# OVB Examples

```{r}
library(tidyverse)
df_educ <- data_frame(iq = runif(500, min = 1, max = 100),
                      school_years = round(16 - 0.05*iq + rnorm(500)),
                      income = 20000 + 5000*school_years + 5000*rnorm(500) + 500*iq)
ggplot(df_educ, aes(x = factor(school_years), y = income)) + geom_boxplot()
```

---
# OVB Examples

```{r}
summary(lm(income ~ school_years, data = df_educ))
summary(lm(income ~ school_years + iq, data = df_educ))
```

---
# OVB Thought Examples

what might be omitted in observational data?

- effect of air quality policy
- effect of advertising campaign
- effect of customer loyalty campaign
- adoption of a mobile app
- calling customer service
- effect of changing a search algorithm

---
# Selection Bias + External Validity

Selection bias falls into two camps:

1. self-selection into treatment (schooling, job training, email campaign)

2. sometimes we're OK with this though, it can sometimes estimate the "treatment effect on the treated"
  - still need to worry about other omitted variables though
  - external vs internal validity, "average" causal effect
  - depends on the goal of the causal analysis

---
# Regression Functional Form Bias

- non-linear outcome variables
  - wages is usually not linear in the outcomes
    - we usually use $\log (wage)$ instead!
    - $\log(wage) ~ \beta educ$, now $\beta$ is interpreted as a percent increase in wage
- sometimes our features are non-linear too $\beta_i age + \beta_j age^2$
- 0/1 outcome variable?
  - logistic regression is common
  - Probit regression has a more economic interpretation, based on an individual's response to 
  - Poisson regression for counts
- Truncated/Censored Observation (e.g., test scores, wage)
- Attrition bias

- *important to think critically about where the (potential) data comes from or would come from*
- *almost all interesting data comes from human decisions, which is complex and may have several things affecting it*

---
# Extensions to Linear Form

- interactions: $\beta_j X_j \cdot X_k$
- feature transformations: $log(Y_i)$
- flexible semi-parametric: $Y_i = \beta_0 + \gamma T_i + \beta f(\mathbf{X_i}) + \epsilon_i $


---
# Example: Regression Functional Form

```{r}
df <- read_dta('../datasets/ak_91_iv_qob.dta') %>%
  mutate(weekly_wage = exp(lnw))
year_df <- df %>% group_by(s) %>%
  summarise(mean_wage = mean(weekly_wage))
g_CEF <- ggplot(year_df, aes(x = s, y = mean_wage)) + 
  geom_point() + 
  geom_smooth(span = 0.35, se = F, color = 'blue', method = 'loess') +
  geom_smooth(method = 'lm', se = F, color = 'red') +
  ggtitle("CEF between education & weekly wage (with both complex & simple fit)")
g_CEF
```

---
# Example: Regression Functional Form

```{r}
set.seed(10)
bad_fit_lm <- lm(weekly_wage ~ s, data = sample_n(df, size = 2000))
plot(bad_fit_lm, which = 1)
plot(bad_fit_lm, which = 5)
```


---
# OVB Illustrated

![illustrated_confound](./assets/iv_confound.png)

---
# Despite Caution, Causal Work Still Useful

- Raj Chetty, policy impact
- Work on piracy affecting movie studios
- Health insurance studies
- Air quality impact studies

---
# Reference Sources

- Cameron & Triveti textbook
- Mostly Harmless Econometrics
- Osea Giuntella, slides