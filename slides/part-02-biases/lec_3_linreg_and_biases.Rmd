---
title: "Causal Inference for Data Science"
subtitle: "ITAM Short Workshop"
author: "Mathew Kiang, Zhe Zhang, Monica Alexander"
date: "March 15, 2017"
output:
  xaringan::moon_reader:
    css: ["custom.css", "./../custom.css", ]
    # In order for the css file to work, you need to set your working directory
    # to one above the slide directory ('./../') and then call moon_reader via
    # `xaringan::inf_mr('./part-01-intro/index.Rmd')`
    # Or just knit it into a browser and it should work immediately.
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
name: inverse
layout: true
class: center, middle, inverse

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

---

# Statistics of Causal Inference

- causal interpretation of linear regression
- possible causal biases / "mis-identification"
- linear regression is not restrictive

???
Linear regression is interpretable. We care about unbiased interpretations rather than predictive fit. These overlap, but the former is most important.

---
# Example Data

![CEF_example](./assets/mhe_cef.png)
??
In this example, we have years of education on the x-axis and wage earnings on the y-axis.
The grey areas represent the distribution of data from each person with that wage.
We're clearly not going to try to explain such a variable thing. 
Instead, we start with the simple idea that there is clear causal pattern due to years of education.

---

# Review of Linear Regression

- (continuous) outcome is additive in the features
$$Y_i = \beta_0 + \beta \mathbf{X_i} + \epsilon_i$$
--
$$Y_i = \beta_0 + \gamma T_i + \beta \mathbf{X_i} + \epsilon_i$$

- not limited to simple linear possibilities (interactions, feature transformation, semi-parametric methods)

??
- Going to spend a lot of time thinking about $\gamma$ and $\epsilon_i$.
- Though we will just use regression, this does not limit our scope much:

---
# Causal Linear Regression

- What we really care about:
$$ E[Y|X=x_i] $$

![CEF_regression](./assets/mhe_cef_regression.png)

---
# Recreate Graphic

```{r, message=F, error=FALSE, warning=F}
library(tidyverse); library(haven)
df <- read_dta('../datasets/ak_91_iv_qob.dta')
year_df <- df %>% group_by(s) %>%
  summarise(mean_lw = mean(lnw))
ggplot(year_df, aes(x = s, y = mean_lw)) + geom_point() +
  geom_line()
```


---

# Causal Regression

- conditional expectation function (CEF) is argued to be causal
- controlling for observed differences
- estimating systematic randomness; 
  - we know we can't explain (close to) everything
  - instead, we want *unbiased* estimates of particular causal patterns

??
We know that it's not that simple to explain complex real-world outcomes with a linear model.
However, if we believe we have an unbiased estimate, this is helpful approximation. The exact numbers usually aren't that crucial.
And linear models are fairly robust.

---

# Regression Maths

- True CEF (best estimate for $Y_i$): $$Y_i = E[Y_i|X=X_i]+\epsilon_i$$
- Linear regression estimates the best MSE **linear** approximation for $E[Y_i|X_i]$
  - how does the distribution of $Y_i$ change *wrt* $X_i$?
  - linear regression gives us interpretable coefficients
- Only the average causal effect
  
---

# Regression Maths (in Potential Outcomes)

- [todo] copy from MKiang?

---

# Note on Causal Language

- be careful using: {"effect", "leads to", "results in", "because of"}
- instead when just observed patterns {"related with", "pattern", "correlation", "tends to", "observed"}

---

# Conditional Independence ("controls")

- getting a Master's degree may not be appropriately random
- people who are prescribed a drug treatment may not be comparable to general population

- it could be argued though:
  - students with similar GPA, age, and Bachelor degrees may be random
  - people with similar blood measurements seen by different doctors may have random drug treatment
  
- ${Y_{0i}, Y_{1i}} \bot C_i ?$

--

- ${Y_{0i}, Y_{1i}} \bot C_i | \mathbf{X_i}?$
- $Y_{si} \bot s_i | \mathbf{X_i}\quad\forall s?$

---

# Avg Causal Effect

$$
\begin{align*}
\ & E[Y_{i}|X_{i},s_{i}=s]-E[Y_{i}|X_{i},s_{i}=s-1]\\
 & =E[Y_{si}-Y_{(s-1)i}|X_{i}]
\end{align*}
$$

--
- but there's an $X_i$ there?

# Linear Regression with CIA

- linear additive assumption simplifies things

$$
\begin{align*}
\ \gamma_{s_i} & =E[Y_{i}|X_{i},s_{i}=s]-E[Y_{i}|X_{i},s_{i}=s-1]\\
 & =E[Y_{si}-Y_{(s-1)i}] + \beta X_i - \beta X_i\\
 & =E[Y_{si}-Y_{(s-1)i}]
\end{align*}
$$

---

# Possibles Biases (many!)

observational data

(almost) All data is observational.

If you think you have an experiment, it's probably still observational data.

- omitted variable bias
- selection bias
- regression form bias

??
OVB and selection bias are similar

---
# Omitted Variable Bias

when Conditional Independece is violated

- omitted variables = (uncontrolled) correlation with the $T_i$ 
- leads the error term $\epsilon_i$ to affect estimate of $\gamma$
- **confounding**

???
- most common occurrence in analysis, i.e. OVB
- up to analyst to decide if OVB affects insights
- occurs when omitted variable affects both the outcome and a predictor

---
# OVB Illustrated

[todo] replace this

![illustrated_confound](./assets/iv_confound.png)

---
# OVB Examples

![ovb_education](./assets/ovb_estimates.png)

---
# OVB Examples

```{r}
library(tidyverse)
df_educ <- data_frame(iq = runif(200, 1, 100),
                      school_years = 16 - 0.05*iq + rnorm(200),
                      income = 20000 + 5000*school_years + 5000*rnorm(200) + 500*iq)
summary(lm(income ~ school_years, data = df_educ))
summary(lm(income ~ school_years + iq, data = df_educ))
```


---
# What does it mean to "control for covariates"?

- makes observed outcomes independent of our causal treatment/action of interest
- based on theory only though; there can always be omitted variable bias
  - and bias caused by bad controls
  - and functional form biases

---
# Omitted Variables

- we rarely "prove" causality
- instead, we argue based on domain expertise and theory that we are not facing biases
- often, a good analysis adjusts a lot of the assumptions to ensure the results are "robust" to possible biases


---
# OVB Thought Examples

what observational data is available?

- effect of air quality policy
- effect of advertising campaign
- effect of OTC drug campaign
- effect of customer loyalty campaign
- adoption of a mobile app
- calling customer service


---
# Selection Bias

(usually, this falls under omitted variable bias)

- sometimes selection bias can be also important for external validity
- "average" causal effect may be poorly estimated 

---
# Regression Functional Form Bias

- non-linear outcome variables
  - wages is usually not linear in the outcomes
    - we usually use $\log (wage)$ instead!
    - $\log(wage) ~ \beta educ$, now $\beta$ is interpreted as a percent increase in wage
- sometimes our features are non-linear too $\beta_i age + \beta_j age^2$
- 0/1 outcome variable?
  - logistic regression is common
  - Probit regression has a more economic interpretation, based on an individual's response to 
  - Poisson regression for counts
- Truncated/Censored Observation (e.g., test scores, wage)
- Attrition bias

- *important to think critically about where the (potential) data comes from or would come from*
- *almost all interesting data comes from human decisions, which is complex and may have several things affecting it*

---
# Extensions to Linear Form

- interactions: $\beta_j X_j \cdot X_k$
- feature transformations: $log(Y_i)$
- flexible semi-parametric: $Y_i = \beta_0 + \gamma T_i + \beta f(\mathbf{X_i}) + \epsilon_i $

---
# Despite Caution, Causal Work Still Useful

- Raj Chetty, policy impact
- Work on piracy affecting movie studios
- Health insurance

---
# Sources

- Cameron & Triveti textbook
- Mostly Harmless Econometrics
- Osea Giuntella, slides