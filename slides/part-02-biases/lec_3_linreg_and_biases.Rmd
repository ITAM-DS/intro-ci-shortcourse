---
title: "Causal Inference for Data Science"
subtitle: "ITAM Short Workshop"
author: "Mathew Kiang, Zhe Zhang, Monica Alexander"
date: "March 15, 2017"
output:
  xaringan::moon_reader:
    css: ["custom.css", "./../custom.css", ]
    # In order for the css file to work, you need to set your working directory
    # to one above the slide directory ('./../') and then call moon_reader via
    # `xaringan::inf_mr('./part-01-intro/index.Rmd')`
    # Or just knit it into a browser and it should work immediately.
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
name: inverse
layout: true
class: center, middle, inverse

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

---

# Statistics of Causal Inference

- linear regression; with causal interpretation
- possible causal biases / "mis-identification"
- linear regression is not a restrictive methodology

???
We're going to cover these three topics.
First, Matt introduced the importance of casual identification and causal questions.
He also emphasized the Rubin Causal Model (potential outcomes framework).
We'll return to this throughout the next few lectures.
In this lecture, we talk about the basic statistical tools we use to identify causal estimates. Specifically in much of the literature, it is linear regression.

In ML and data science, usually we like linear regression for a first attempt, but it's not really a "hot" model to use in general. This is because it usually is not the best predictor. However, in this case, the best prediction is not what we care about.

Instead, we want to be able to identify a causal estimate. And an unbiased causal estimate.

Linear regression is interpretable. We would like better prediction, but that's usually not the first priority. Let's take this next image as an example.

---
# Statistical Basics

- $Y$ = Outcome of Interest
- $T$ = Treatment
- $E[Y|T]$ = Conditional Expectation Function

First, we discuss the CEF; second, when is the CEF causal?

---
# Predicting Job Earnings

![CEF_example](./assets/mhe_cef.png)

- What about all the extra noise in wages?
--

- What does the above line mean to us?

???
In this example, we have years of education on the x-axis and wage earnings on the y-axis.
The grey areas represent the distribution of data from each person with that wage.
We're clearly not going to try to explain such a variable thing. 

Instead, we're looking at the systematic pattern between education and wages.

---
# Recreate Graphic

```{r, message=F, error=FALSE, warning=F}
library(tidyverse); library(haven)
df <- read_dta('../datasets/ak_91_iv_qob.dta') %>%
  mutate(weekly_wage = exp(lnw))
year_df <- df %>% group_by(s) %>%
  summarise(mean_lw = mean(lnw))
g_CEF <- ggplot(year_df, aes(x = s, y = mean_lw)) + 
  geom_point() + 
  geom_smooth(span = 0.35, se = F, color = 'blue', method = 'loess') +
  geom_smooth(method = 'lm', se = F, color = 'red') +
  ggtitle("CEF between education & log-wage (with both complex & simple fit)")
```

---
# Recreate Graphic

```{r}
g_CEF
```

---
# Linear Regression Estimates a Linear CEF

CEF generally: $Y = E[Y|T,X] + \epsilon$

- basic interpretation: assume $E[Y|T,X]$ is **linear**

$$\hat{Y_i} = [\hat{\beta}_0 + \hat{\gamma} T_i] + \hat{\epsilon_i}$$
--

$$\hat{Y_i} = [\hat{\beta}_0 + \hat{\gamma} T_i + \hat{\mathbf{\beta}} \mathbf{X_i}] + \hat{\epsilon_i}$$

*Note: not the only interpretation*
*Note: linear form is not that restrictive*

???
This is the best MSE predictor, no matter. What we need to do is to find a functional form of the explained part.
By definition, Epsilon is uncorrelated and mean-zero. 

- In the usual linear regression, we only interpret the beta coefficients as helping to use X to explain variation in Y.
Now, we'll include a causal effect of interest, gamma, and make sure that we're estimating gamma correctly. This will be the crux of what we're focusing on in this class.

- Going to spend a lot of time thinking about $\gamma$ and $\epsilon_i$.
- Though we will just use regression, this does not limit our scope much:
not limited to simple linear possibilities (interactions, feature transformation, semi-parametric methods)


---
# Causal Linear Regression, Causal CEF

So far, this applies for all linear regression.  
When can we argue that the CEF is *causal*?

*Does the CEF have (conditional) indepedence from the potential outcomes?*

--

Consider observations $Y_i, T_i$ where T_i is a binary treatment.

$$\{ Y_{0i}, Y_{1i} \}$$

$$Y_i = Y_{0i} + (Y_{1i}-Y_{0i})T_i$$

Simple linear regression: $Y_i \sim \gamma T_i + \epsilon_i$

$$avgEffect = \hat{\gamma} = E[Y_i|T_i=1] - E[Y|T_i=0]$$ 

---
# Causal Linear Regression, Causal CEF

Simple linear regression: $Y_i \sim \gamma T_i + \epsilon_i$

$$avgEffect = \hat{\gamma} = E[Y_i|T_i=1] - E[Y|T_i=0]$$ 

Is this causal?

--

$$ CIA \equiv \left\{ Y_{0i}, Y_{1i} \right\} \bot T_i$$

To see why:

$$avgEffect = \hat{\gamma} = E[Y_i|T_i=1] - E[Y|T_i=0]$$ 

$$\hat{\gamma} = E[Y_{1i} - Y_{0i} | T_i = 1] - \left\{ E[Y_{0i}|T_i=1] - E[Y_{0i}|T_i=0] \right\}$$

The second term must be 0.
*The second term is also what will most difficult to convince people of.*

*Note: average treatment effect*


???
This is the potential outcomes change that we want.
And this is the indepdence that we want.

---
# Conditional Independence Assumption (CIA)

Previous example works for an RCT.
- built-in conditional indepdence assumption (CIA)

In observation data, we usually need to manipulate data to get CIA
- "conditional on other covariates, $\mathbf{X_i}$, we get CIA"

$$\left\{ Y_{0i}, Y_{1i} \right\} \neg\bot T_i \qquad \left\{ Y_{0i}, Y_{1i} \right\} \bot T_i|\mathbf{X_i}$$

--

$$\hat{Y_i} = [\hat{\beta}_0 + \hat{\gamma} T_i + \hat{\mathbf{\beta}} \mathbf{X_i}] + \hat{\epsilon_i}$$

*e.g. conditioning on neighborhood wealth, whether you go to a 'good' school or a 'bad' school is independent of your counterfactuals*

???
Obvious to avoid that good people go to good schools.
Also, avoid that people who are bad go to good schools.

---
# What about non-binary treatments?

Linearity assumption makes things simpler.

![CEF_example](./assets/mhe_cef_regression.png)

$$CIA = \left\{ Y_{t,i}, Y_{(t-1),i} \right\} \bot t_i|\mathbf{X_i} \quad \forall t$$

Do we need to estimate a causal effect for each possible value of $\mathbf{X_i}$?

$$E[Y_i|T=T_a,X_i] - E[Y_i|T=T_b,X_i] = \gamma (T_a - T_b) + (\beta X_i - \beta X_i)$$

(a) No, we can focus on the overall average treatment effect.
(b) simple, robust interpretation: assumes that $E[Y|T]$ is linear.
- Then, in the linear regression, $\mathbf{X_i}$ controls for the other variation

In other notation, the linear assumption gives us:
$$\hat{\gamma} = \frac{\partial{Y}}{\partial{T}}$$

???
Important to be cognizant that this is what you're estimating however.

---

# Causal Regression Interpretation

- estimating systematic randomness; 
  - we know we can't explain (close to) everything
  - instead, we want *unbiased* estimates of particular causal patterns
- argue that the conditional expectation function (CEF) is causal

??
We know that it's not that simple to explain complex real-world outcomes with a linear model.
We need to argue using using theory that our estimate of the CEF is okay.

However, if we believe we have an unbiased estimate, this is useful approximation. The exact numbers usually aren't that crucial. 
And linear models are fairly robust.


---

# Best Linear Approximation to CEF

- how does the distribution of $Y_i$ change *wrt* $X_i$?
- linear regression gives us interpretable coefficients
- Only the average causal effect

- True $\gamma, \epsilon$ are unknown. Instead, we estimate using observations $i$.

???
Even if truly non-linear, there is value in having a robust linear approximation.

---
# Concerns in the $\epsilon$ term

Recall our discussion of RCTs.

$$E[Y_i|T=T_a,X_i] - E[Y_i|T=T_b,X_i] = \\ \gamma (T_a - T_b) + \\ E[\epsilon_i|T_a] - E[\epsilon_i|T_b]$$

$$E[Y_i|T=T_a] - E[Y_i|T=T_b] = \\ \gamma (T_a - T_b) + \\ (0 - 0)$$


---
# (Causal) Linear Regression Gone Wrong

$$E[Y_i|T=T_a] - E[Y_i|T=T_b] = \\ \gamma (T_a - T_b) + \\ E[\epsilon_i|T_a] - E[\epsilon_i|T_b]$$

What if?:

$$E[\epsilon_i|T_a] >> E[\epsilon_i|T_b]$$

Then:

$$E[Y_i|T=T_a] - E[Y_i|T=T_b] = \gamma (T_a - T_b) + Bias_{OVB}$$

---
# Example of OVB

# (Causal) Linear Regression Gone Wrong

$$E[Y_i|Loc_A] - E[Y_i|Loc_B] = \\ \gamma + \\ E[\epsilon_i|Loc_A] - E[\epsilon_i|Loc_B]$$

$\epsilon_i$ could be higher in $Loc_A$ because of the wealthy parents and peers effect, while $\epsilon_i$ could be lower in $Loc_B$ because of nearby crime and travel time.

- Occurs because assumed linear model is wrong. The true model may be $Y = \gamma Loc_A + \beta wealth_{parents} + \epsilon$.

---
# Omitted Variable Bias Confounding

- true correlation between {$T_i$, $\epsilon$} (not $\hat{\epsilon}$) causes problems
- AKA: **omitted variable bias**, **confounding**

---

# Note on Using Causal Terminology

- causal: {"effect", "leads to", "results in", "because of", "impact"}
- observed patterns: {"related with", "pattern", "correlation", "tends to move with", "observed"}

---
# OVB Examples

![ovb_education](./assets/ovb_estimates.png)

---
# OVB Examples

```{r}
library(tidyverse)
df_educ <- data_frame(iq = runif(200, 1, 100),
                      school_years = 16 - 0.05*iq^1.1 + rnorm(200),
                      income = 20000 + 5000*school_years + 5000*rnorm(200) + 500*iq)
summary(lm(income ~ school_years, data = df_educ))
summary(lm(income ~ school_years + iq, data = df_educ))
```

---
# Omitted Variables

- we rarely "prove" causality
- instead, we argue based on domain expertise and theory that we are not facing biases
- often, a good analysis adjusts a lot of the assumptions to ensure the results are "robust" to possible biases

---
# OVB Thought Examples

what observational data is available?

- effect of air quality policy
- effect of advertising campaign
- effect of OTC drug campaign
- effect of customer loyalty campaign
- adoption of a mobile app
- calling customer service


---
# Selection Bias

(usually, this falls under omitted variable bias)

- sometimes selection bias can be also important for external validity
- "average" causal effect may be poorly estimated 

---
# Regression Functional Form Bias

- non-linear outcome variables
  - wages is usually not linear in the outcomes
    - we usually use $\log (wage)$ instead!
    - $\log(wage) ~ \beta educ$, now $\beta$ is interpreted as a percent increase in wage
- sometimes our features are non-linear too $\beta_i age + \beta_j age^2$
- 0/1 outcome variable?
  - logistic regression is common
  - Probit regression has a more economic interpretation, based on an individual's response to 
  - Poisson regression for counts
- Truncated/Censored Observation (e.g., test scores, wage)
- Attrition bias

- *important to think critically about where the (potential) data comes from or would come from*
- *almost all interesting data comes from human decisions, which is complex and may have several things affecting it*

---
# Example: Regression Functional Form

```{r}
df <- read_dta('../datasets/ak_91_iv_qob.dta') %>%
  mutate(weekly_wage = exp(lnw))
year_df <- df %>% group_by(s) %>%
  summarise(mean_wage = mean(weekly_wage))
g_CEF <- ggplot(year_df, aes(x = s, y = mean_wage)) + 
  geom_point() + 
  geom_smooth(span = 0.35, se = F, color = 'blue', method = 'loess') +
  geom_smooth(method = 'lm', se = F, color = 'red') +
  ggtitle("CEF between education & weekly wage (with both complex & simple fit)")
g_CEF
```


---

# Avg Causal Effect

$$
\begin{align*}
\ & E[Y_{i}|X_{i},s_{i}=s]-E[Y_{i}|X_{i},s_{i}=s-1] \\
\ & =E[Y_{si}-Y_{(s-1)i}|X_{i}]
\end{align*}
$$

- linear additive assumption simplifies things

$$
\begin{align*}
\ \gamma_{s_i} & =E[Y_{i}|X_{i},s_{i}=s]-E[Y_{i}|X_{i},s_{i}=s-1]\\
 & =E[Y_{si}-Y_{(s-1)i}] + \beta X_i - \beta X_i\\
 & =E[Y_{si}-Y_{(s-1)i}]
\end{align*}
$$

---

# Possibles Biases (many!)

observational data

(almost) All data is observational.

If you think you have an experiment, it's probably still observational data.

- omitted variable bias
- selection bias
- regression form bias
- measurement bias
- reverse causality bias

??
OVB and selection bias are similar

---
# OVB Illustrated

[todo] replace this

![illustrated_confound](./assets/iv_confound.png)


---
# Extensions to Linear Form

- interactions: $\beta_j X_j \cdot X_k$
- feature transformations: $log(Y_i)$
- flexible semi-parametric: $Y_i = \beta_0 + \gamma T_i + \beta f(\mathbf{X_i}) + \epsilon_i $

---
# Despite Caution, Causal Work Still Useful

- Raj Chetty, policy impact
- Work on piracy affecting movie studios
- Health insurance

---
# Sources

- Cameron & Triveti textbook
- Mostly Harmless Econometrics
- Osea Giuntella, slides