---
title: "Causal Inference for Data Science"
subtitle: "ITAM Short Workshop"
author: "Mathew Kiang, Zhe Zhang, Monica Alexander"
date: "March 14, 2017"
output:
  xaringan::moon_reader:
    css: ["custom.css", "./../custom.css", ]
    # In order for the css file to work, you need to set your working directory
    # to one above the slide directory ('./../') and then call moon_reader via
    # `xaringan::inf_mr('./part-01-intro/index.Rmd')`
    # Or just knit it into a browser and it should work immediately.
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
name: inverse
layout: true
class: center, middle, inverse

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

---

# Statistics of Causal Inference

- why linear regression?
- overview of linear regression checks
- possible biases in linear regression

???
Linear regression is interpretable. We care about unbiased interpretations rather than predictive fit. These overlap, but the former is most important.


---

# Review of Linear Regression

- outcome is additive in the provided features
- $Y_i = \beta_0 + \beta \mathbf{X_i} + \epsilon_i $
- $Y_i = \beta_0 + \gamma T_i + \beta \mathbf{X_i} + \epsilon_i $

- Going to spend a lot of time thinking about $\gamma$ and $\epsilon_i$.
- Though we will just use regression, this does not limit our scope much:
- interactions: $\beta_j X_j \cdot X_k$
- feature transformations: $log(Y_i)$
- flexible semi-parametric: $Y_i = \beta_0 + \gamma T_i + \beta f(\mathbf{X_i}) + \epsilon_i $

---

# Causal Regression

- conditional expectation function is assumed to be causal
- controlling for observed differences
- looking for systematic randomness; we know we can't explain (close to) everything, instead we want to get *unbiased* estimates of specific causal patterns of interest.
  
![CEF_example](./assets/mhe_cef.png)
  
  
---

# Regression Maths

- True CEF (best estimate for $Y_i$): $$Y_i = E[Y_i|X_i]+\epsilon_i$$
- Linear regression estimates the best MSE linear approximation for $E[Y_i|X_i]$
  - how does the distribution of $Y_i$ change *wrt* $X_i$?
  - linear regression gives us interpretable coefficients

![CEF_regression](./assets/mhe_cef_regression.png)

---

# Regression in Potential Outcomes

- [todo] left open

---

# Causal Language

- be careful using: {"effect", "leads to", "results in", "because of"}
- instead when just observed patterns {"related with", "pattern", "correlation", "tends to", "observed"}

---

# Conditional Independence

- getting a Master's degree may not be appropriately random
- people who are prescribed a drug treatment may not be comparable to general population

- it could be argued though:
  - students with similar GPA, age, and Bachelor degrees may be random
  - people with similar blood measurements seen by different doctors may have random drug treatment
  
- ${Y_{0i}, Y_{1i}} \bot C_i ?$
- ${Y_{0i}, Y_{1i}} \bot C_i | \mathbf{X_i}?$
- $Y_{si} \bot s_i | \mathbf{X_i}\quad\forall s?$

---

# Avg Causal Effect

$$
\begin{align*}
\ & E[Y_{i}|X_{i},s_{i}=s]-E[Y_{i}|X_{i},s_{i}=s-1]\\
 & =E[Y_{si}-Y_{(s-1)i}|X_{i}]
\end{align*}
$$
--
- but there's an $X_i$ there?

# Linear Regression with CIA

- linear additive assumption simplifies things

$$
\begin{align*}
\ \gamma_{s_i} & =E[Y_{i}|X_{i},s_{i}=s]-E[Y_{i}|X_{i},s_{i}=s-1]\\
 & =E[Y_{si}-Y_{(s-1)i}] + \beta X_i - \beta X_i\\
 & =E[Y_{si}-Y_{(s-1)i}]
\end{align*}
$$

---

# Biases: when Conditional Independece is violated

- omitted variables: correlation with the $T_i$ leads the error term to affect the estimate of $\gamma$
- skip the math;
- conceptually, 

![ovb_education](./assets/ovb_estimates.png)


---

# Biases in Observational Data

???

(almost) All data is observational.

If you think you have an experiment, it's probably still observational data.

---

# Roadmap

4 major types of bias:
- *omitted variable confounding*
- selection
- truncation/censored variables
- choice of regression error term

---
layout: false
.left-column[
  ## Roadmap
  ### Omitted Variable Confounding
]
.right-column[
<br><br><br>
- most common occurrence in analysis, i.e. OVB
- up to analyst to decide if OVB affects insights
- occurs when omitted variable affects both the outcome and a predictor
]

---
layout: false
.left-column[
  ## Roadmap
  ### Omitted Variable Confounding
]
.right-column[
- most common occurrence in analysis, i.e. OVB
- up to analyst to decide if OVB affects insights
- occurs when omitted variable affects both the outcome and a predictor
![Wired Article](./assets/dag_ovb_simple.png)
]

---
layout: false
.left-column[
  ## Roadmap
  ### Omitted Variable Confounding
  #### Examples
]
.right-column[
```{r}
library(tidyverse)
df_educ <- data_frame(iq = runif(200, 1, 100),
                      school_years = 16 - 0.05*iq + rnorm(200),
                      income = 20000 + 5000*school_years + 5000*rnorm(200) + 500*iq)
summary(lm(income ~ school_years, data = df_educ))
summary(lm(income ~ school_years + iq, data = df_educ))
```

]


---
layout: false
.left-column[
  ## Roadmap
  ### Omitted Variable Confounding
  #### Academic Examples
]
.right-column[
- job training programs [cite] [todo]
- schooling
]

---
layout: false
.left-column[
  ## Roadmap
  ### Omitted Variable Confounding
  #### Data Science Examples
]
.right-column[
- advertising tracking
- adoption of a mobile app
- calling customer service
]

---
# Omitted Variables

- we rarely "prove" causality
- instead, we argue based on domain expertise and theory that we are not facing biases
- often, a good analysis adjusts a lot of the assumptions to ensure the results are "robust" to possible biases

---
# Instrumental Variables

- any other variables in the world or data that helps to encourage a real-world experiment
- by leveraging this information, we can remove omitted variable bias
- denote such instruments with $z_i$ variables for each observation $i$

---
# Instrumental Variables Graphically

- replace with our own graphics for consistent letters

![iv_base](./assets/iv_no_issue.png) ![iv_base](./assets/iv_confound.png)
![iv_base](./assets/iv_with_z.png)

---
# Instrumental Variable Key

- instead of using the observed $X_i$ or $T_i$, which may be influenced by confounders,
we use a predicted version of $\hat{X_i}$, which does not suffer from influence from confounders.

- "good instruments come from a combination of institutional knowledge and ideas about the processes determining the variables of interest" (Angrist & Pischke)
  - i.e., it's not just about data
  
---
# Various Types of Instruments

- natural experiments
- policy variation
- policy/implementation details
- actions taken by third parties

---
# Various Types of Instruments

- examples of and papers using different types 

---
# Caveats about IV

- we can never "prove" that an IV is valid, it has to be argued
- we can show that an IV is weak sometimes though
- beware of internal validity causality; LATE
  - causality is determined based on meaningful variation (e.g., randomized experiment); an IV may only lead to meaningful variation for a small part of the population (e.g., RD)
  - can help inform our theoretical understanding
  - comes up when we consider the more realistic scenario that casual effects are heterogeneous

---
# LATE

- only estimate the avg causal effect for those who are affected
- those who are affected by all be affected in the same direction; otherwise it complicates calculation
  - this is a hypothetical/theoretical assumption, we can never know
- "IV solves the problem of causal inference in a randomized trial with partial compliance" (Angrist, Pischke)

---
# Intention to Treat in Randomized Treatments

- some people who choose to decline a randomized offer/treatment
- use the offer variable as an IV

---
# Workshop Brainstorm

- imagine you work at Google/Federal Government/Facebook and you want to estimate ???
- think of possible instruments?

---
# Sources

- Cameron & Triveti textbook
- Mostly Harmless Econometrics
- Osea Giuntella, slides