<!DOCTYPE html>
<html>
  <head>
    <title>Evaluating causality in an ideal setting</title>
    <meta charset="utf-8">
    <meta name="author" content="Mathew Kiang, Zhe Zhang, Monica Alexander" />
    <meta name="date" content="2017-03-15" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
    <link rel="stylesheet" href="../custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Evaluating causality in an <em>ideal</em> setting
## RCTs, DAGs, and Bias
### Mathew Kiang, Zhe Zhang, Monica Alexander
### March 15, 2017

---



&lt;!--
class: center, middle


.center[&lt;img src="./assets/do_you_like_dags.jpg"&gt;]
--&gt;

# Roadmap

???
`\(\def\indep{\perp \! \! \perp}\)`

So we've got some of the basic definitions down. We know what a causal effect is and we have a framework for estimating causal effects under certain assumptions. Now we're going to talk about the best case scenario for estimaing causal effects — RCTs. **NEXT SLIDE**

While you probably won't get to do a lot of randomized experiments in the real world, they are useful to think about because ultimately every study design or analytic method in causal inference is ultimately just trying to mimic an RCT. **NEXT SLIDE**

After talking a bit about RCTs, we'll take a bit of a detour and talk about ways you can graphically state your causal and statistical assumptions. Causal DAGs. I think they're very useful and hope I can convince you they are too; however, a lot of people don't. Regardless, you should know about them because you'll likely come across them in the causal inference literature. **NEXT SLIDE**

Finally, we'll combine RCTs and DAGs to show how bias can affect estimates when we don't have an RCT (and why RCTs are so powerful). Then next lecture, Zhe will talk more about the math behind these different biases.

--

### Randomized Control Trials

- Thinking about causality in the ideal setting
--

### Causal Directed Acyclic Graphs (DAGs)

- Useful ways of encoding our beliefs and assumptions

- Helpful for thinking about ways our assumptions can be wrong
--

### Understanding Bias from a Graphical Perspective

- More about these biases next lecture

- Why RCTs avoid many of these biases


---
# Randomized Control Trials

.pull-right[&lt;img src="./assets/james_lind.jpg" width="250"&gt;]

???

Again, we'll start off with a bit of motivation and history. **NEXT SLIDE** 

This is James Lind. He was a Scottish physician in the Royal Navy and he conducted the first recorded RCT in the 1700s. Specifically, he was interested in scurvy which is a disease from Vitamin C deficiency and causes pretty bad symptoms. It's a disease of the connective tissue because Vitamin C is required to create connective tissue, all sorts of bad things happen. Severe bleeding, gum disease, inability to heal from superficial wounds, etc. People eventually get personality changes and then die of infection or bleeding. That said, it's very easy to treat. Just some Vitamin C and improvement occurs in days and full recovery in about a month. Was very common back when there were long sea voyages and they were unable to keep fresh fruit available. **NEXT SLIDE**

So Lind was on a ship and he found 12 sailors with scurvey. He randomly split them up into 6 groups and gave each group a different treatment in addition to their regular diet. These are the six different interventions. **NEXT SLIDE**

As he suspected, only the group that had citrus fruits showed substantial improvement with one returning back to work and one on the mend (they ran out of fruit before he could completely heal).

--
.footnote[Wikipedia]

- First recorded RCT was done in 1747 by James Lind

--

- Scurvy is a terrible disease caused by Vitamin C deficiency. Serious issue during long sea voyages.

--

- Lind took 12 sailors with scurvy and split them into six groups of two:

    - Groups were assigned: (1) 1 qt cider, (2) 25 drops of vitriol, (3) 6 spoonfuls of vinegar, (4) 1/2 pt of sea water, (5) garlic, mustard, and barley water, (6) 2 oranges and 1 lemon

--

- Only Group 6 (citrus fruit) showed substantial improvement.

---
# Randomized Control Trials
.footnote[Hernan Epi 201]

### What's the big deal about RCTs?

???

There are a few things worth noting about Lind's RCT. **NEXT SLIDE**

First, he showed a strong causal effect with a tiny sample. An n=12 study would literally never get published today, but nonetheless, it's hard to argue with a strong causal effect even in a small RCT. **NEXT SLIDE**

Second, even though Lind was wildly wrong about **why** the RCT worked, he was right about the causal effect of citric acid on preventing and curing scurvy. Back then, medicine didn't reocgnize the importance of vitamins and so Lind believed that survey or all the bleeding was a result of the body beging to putrefy and acid reduced the process.

--

- Even with a very small sample size, Lind was able to show convincing causal effect

--

- RCTs allow us to gain knowledge about causal effects **without knowing the mechanism** 

    - Lind believed acid stopped putrefaction. 
    - Medicine had not yet understood importance of vitamins.

---
# Randomized Control Trials
### But RCTs aren't perfect

.center[&lt;img src="./assets/parachutes_rcts.jpg" width="450"&gt;]

---
# Randomized Control Trials
### But RCTs aren't perfect

- Expensive — limited feasibility

???

However, it is worth noting that you can't always do an RCT. You've never read an RCT on the causal effects of smoking and lung cancer in humans. It would be wildly unethical to force some humans to smoke and prevent others from doing so. The famous example people like to use is that you've never read an RCT on the effectiveness of parachutes on preventing death from jumping out of a plane. We know they work. We have observational studies to indicate it works. We have strong priors. Etc. **NEXT SLIDE**

It's also important to note that RCTs are often very costly and have a smaller, more select sample size. While the *internal* validity is high, the *external* validity may not be. In other words, I know the causal effect is true for people *inside of my study*. But I cannot be certain that it is true for everybody outside of my study as well.

--

- Ethical concerns — real effects on real people 

    - Facebook and emotional manipulation study

--

- Smaller sample size (compared to observational studies) — limited generalizability

    - The *internal validity* of an RCT is very high, but the *external validity* is often not.

--

- Difficult — good RCTs are very hard to do

---
# RCT Definitions

### .red[TODO: SHOULD I TAKE THIS OUT?]

.footnote[Hernan Epi 201]

- **Randomized**: The intervention is assigned randomly

???

--

- **Controlled**: There is (at least) one well-defined comparison group

--

- **Experiment**: Investigators (directly) control the intervention

- **Trial**: An experiment where the goal is to study effect of some medical intervention on humans

    - Trials have ethical requirements that experiments may not (e.g., equipoise)

--
&lt;br&gt;
### In an ideal RCT, there is no loss to follow-up, perfect compliance, only one version of treatment, double (or triple) blind assignment
---
# Randomized Control Trials

- Recall from last lecture, because of the *Fundamental Problem of Causal Inference*, we can never observe both counterfactuals.

--

- Instead, we ask: under what conditions do the data we have properly estimate the data we want? `$$Pr(Y^{a=1})-Pr(Y^{a=0})=Pr(Y|A=1)-Pr(Y|A=0)$$`
--

- Again, recall our assumptions for the Rubin causal model:
--

    - How do we ensure our groups *exchangeable*?
    - That we have *positivity*?
    - And that we have *stability*?

--
### .center[These are all met in an ideal RCT]

---
# Randomized Control Trials

.footnote[Hernan Epi 201]

- Suppose we have a very large (nearly infinite) population of perfect compliers

--

- We divide them into two groups: (1) Group Heads and (2) Group Tails 

--

- We assigned the groups by flipping a (fair) coin

--

- Now, we decide to treat Group Heads and not treat Group Tails 

    - The risk of death in Group Heads is: `\(Pr(Y=1|A=1) = .25\)`

--

- Pretend that instead of doing this, we treated Group Tails and did not treat Group Heads. What would the expected risk of death in Group Tails be?

--

    - Risk of death in Group Tails would still `\(.25\)` if we flipped treatment.

---
class: center, middle
# Exchangeability is a consequence of randomization

## &amp;nbsp;

---
class: center, middle
# Exchangeability is a consequence of randomization

## Unfortunately, we (usually) won't get to randomize

---
# Formal definition of exchangeability
.footnote[Hernan Epi 201]

The counterfactual outcome `\(Y^a\)` is independent of the actual treatment `\(A\)`.
`$$Y^a \indep A \text{ for all } a$$`

--

- Exchangeability is "another causal concept that cannot be represented by associational (statistical) language"

--

### NOTE!
Exchanagebility is not the same as independence. That is, `$$Y^a \indep A \neq Y \indep A$$`


---
class: center, middle
# All design and analytic methods try to mimic randomization in order to "create" exchangeability


---
class: center, middle, inverse
# The Life-changing Magic of DAGs

---
# Causal Directed Acyclic Graphs (DAGs)

.footnote[.red[*]Ch 16 by Maria Glymour in Methods Social Epidemiology]

- Causal DAGs provide a mathematical link between statistical association and causality.

???

Let's take what may seem like a little detour and talk about DAGs. DAGs are directed acyclic graphs. DAGs allow us to visually express our assumptions about causal effects based only on statistical associations. That is, you have some data and you run a regression with a bunch of variables and you get out a bunch of coefficients. Drawing a DAG allows you to very explicitly state the relationship between all those statistical associations and how they lead to some causal effect. **NEXT SLIDE**

Importantly, they allow you to also do the reverse. That is, given a bunch of associations from your regression model, what are the different ways you could have come up with a causal estimate and how are those ways wrong. **NEXT SLIDE**

DAGs have a strong mathematical underpinning and link causality and statistics. If you're interested in learning more about them, definitely check out Causality from Judea Pearl. 

--

- DAGs allow us to easily express the statistical associations of different variables implied by our assumed causal structure 

.center[&lt;img src="./assets/basic_dag.jpg" width="200"&gt;]

--

- Even better, also allow us to do the reverse. Given a set of associations in our data, diagram the possible causal structures that could result in the same associations

.center[&lt;img src="./assets/total_confounding.jpg" width="300"&gt;]

---
# DAGs and Causal Inference

.center[&lt;img src="./assets/basic_dag.jpg" width="200"&gt;]
.footnote[.red[*]Ch 16 by Maria Glymour in Methods Social Epidemiology]

- Graphically, the goal of causal inference is to estimate magnitude and direction of the arrow arrow

--

- Statistically, we can only estimate the magnitude

--

- The edge represents statistical association, which can exist due to one or more of the following reasons:
    1. **Randomness**: Just got lucky
    1. **Causal**: `\(X\)` really does cause `\(Y\)`
    1. **Reverse Causation**: `\(Y\)` actually causes `\(X\)`
    1. **Confounding**: `\(X\)` and `\(Y\)` share a common cause
    1. **Collider/Selection**: `\(X\)` and `\(Y\)` have a common effect that we are conditioning on

---
class: center, middle

# Your job is to figure out which of these reasons is most consistent with the data and rule out all other explanations
    
---
# Rules for drawing a DAG

.center[&lt;img src="./assets/example_dag.jpg" width="200"&gt;]
.footnote[.red[*]Ch 16 by Maria Glymour in Methods Social Epidemiology]

- Time flows left to right (thus arrows always point right)

--

- An arrow implies our belief that something is *causal*. Conversely, the lack of an arrow implies our belief that things are *not* causal

--

- Everything we are concerned with exists in the DAG (all common causes must be in the DAG)

---
# Reading DAGs: Causal assumptions

.center[&lt;img src="./assets/example_dag.jpg" width="200"&gt;]
.footnote[.red[*]Ch 16 by Maria Glymour in Methods Social Epidemiology]

- `\(X\)` and `\(U\)` are direct causes of `\(Y\)`
--

- `\(Y\)` is a direct cause of `\(Z\)`
--

- `\(X\)` and `\(U\)` are *indirect* of causes `\(Z\)` through `\(Y\)`
--

- `\(X\)` and `\(U\)` are not causes of each other
--

- No two variables share a common cause

---
# Reading DAGs: Statistical implications

.center[&lt;img src="./assets/example_dag.jpg" width="200"&gt;]
.footnote[.red[*]Ch 16 by Maria Glymour in Methods Social Epidemiology]

- An edge implies statistical dependence
--

- No statistical dependence if there is a *collider*. That is, when two arrowheads point to same variable (e.g., `\(U \rightarrow Y \leftarrow X\)`)
--

- No statistical dependence if we *condition* or *block* a variable (denoted with a box around that variable)
--

- There is statistical dependence if we block a collider or a *descendent* of that collider

---
# Reading DAGs: Statistical implications
.footnote[.red[*]Ch 16 by Maria Glymour in Methods Social Epidemiology]

.center[&lt;img src="./assets/example_dag.jpg" width="200"&gt;]

- `\(X\)` and `\(Y\)` are statistically dependent
- `\(U\)` and `\(Y\)` are statistically dependent
- `\(Y\)` and `\(Z\)` are statistically dependent
--

- `\(X\)` and `\(Z\)` are statistically dependent (i.e., `\(X \rightarrow Y \rightarrow Z\)`)
- `\(U\)` and `\(Z\)` are statistically dependent (i.e., `\(U \rightarrow Y \rightarrow Z\)`)

---
# Reading DAGs: Statistical implications
.footnote[.red[*]Ch 16 by Maria Glymour in Methods Social Epidemiology]

.center[&lt;img src="./assets/example_dag.jpg" width="200"&gt;]

- `\(X\)` and `\(U\)` are statistically independent (blocked by a collider `\(Y\)`)

--
- `\(X\)` and `\(U\)` are statistically dependent, conditional on `\(Y\)` (conditioning on a collider, unblocks that path)

--
- `\(X\)` and `\(U\)` are statistically dependent, conditional on `\(Z\)` (conditioning on a collider's descendent, unblocks that path)

--
- `\(X\)` and `\(Z\)` are statistically independent, conditional on `\(Y\)` (conditioning on `\(Y\)` blocks the `\(X \rightarrow Y \rightarrow Z\)` path)

--
- Similar to above, `\(U\)` and `\(Z\)` are statistically independent, conditional on `\(Y\)`.

---
# DAGs Assumptions
.footnote[.red[*]Ch 16 by Maria Glymour in Methods Social Epidemiology]

.center[&lt;img src="./assets/example_dag.jpg" width="200"&gt;]

1. **Causal Markov Assumption.** Any variable `\(X\)` is independent of any other variable `\(Y\)` conditional on on the direct causes of `\(X\)` (unless `\(Y\)` is an effect of `\(X\)`).
1. **Faithfulness.** Positive and negative causal effects never *perfectly* offset. 
1. **Neglible randomness.** Statistical associations (or lack of such associations) are never due to random change (i.e., large sample size assumption).

---
class: center, middle

# Understanding Bias Graphically
## Structural underpinnings of bias

???

I'm going to go over the ways our models can be wrong using DAGS. It's really just sort of a high level conceptual view of bias and causal inference. Then Zhe will talk about bias in a more mathematical way.
---
# The Structure of Bias

.center[&lt;img src="./assets/partial_confounding.jpg" width="200"&gt;]

- Recall the five ways we can find statistical association:
    
    1. **Randomness**: Just got lucky
    1. **Causal**: `\(X\)` really does cause `\(Y\)`
    1. **Reverse Causation**: `\(Y\)` actually causes `\(X\)`
    1. **Confounding**: `\(X\)` and `\(Y\)` share a common cause
    1. **Collider/Selection**: `\(X\)` and `\(Y\)` have a common effect that we are conditioning on

---
# The Structure of Bias

.center[&lt;img src="./assets/partial_confounding.jpg" width="200"&gt;]

- Recall the five ways we can find statistical association:
    
    1. ~~**Randomness**: Just got lucky~~
    1. **Causal**: `\(X\)` really does cause `\(Y\)`
    1. **Reverse Causation**: `\(Y\)` actually causes `\(X\)`
    1. **Confounding**: `\(X\)` and `\(Y\)` share a common cause
    1. **Collider/Selection**: `\(X\)` and `\(Y\)` have a common effect that we are conditioning on

- Ignore (1) because of negligible randomness assumption (in other words, assume we have a sufficient sample size).

---
#  Reverse Causation

- In observational data, we often do not have *temporal ordering*; that is, we don't know for sure our treatment occurred before our outcome. 

- For example, the causal question "Does income affect health?" has ambiguous temporal ordering. 

- Perhaps people who are sick a lot missed a lot of school and thus have lower paying jobs.

.center[&lt;img src="./assets/reverse_single.jpg" width="200"&gt;]

- "Reverse causality" is a more general term for bias. It is usually a special case of *confounding* or *collider bias*.

---
#  Reverse Causation: Example

- In reality, it is often a mix of both and causation is bidirectional. 

.center[&lt;img src="./assets/reverse_double.jpg" width="200"&gt;]

--

- Healthier people tend to have characteristics that also lead to better health (relative to sicker people). 

- Higher income also allows you to live a healthier lifestyle (e.g., gym membership, health insurance, quality food, etc.)

--

- **Obesity paradox**: People with high BMI seem to have better outcomes after heart attacks than those with normal BMI.

--

- One explanation has been reverse causality — people who are very ill tend to lose weight (and are also at higher risk of death).

---
#  Confounding

.footnote[.red[*]Ch 16 by Maria Glymour in Methods Social Epidemiology]
- When a causal relation between two variables differs from their statistical association, the relationship is **confounded**.

- For example, here the relationship between `\(X\)` and `\(Y\)` does not exist causally, but there is a statistical association induced by *total* confounding through `\(U\)`.

.center[&lt;img src="./assets/total_confounding.jpg" width="200"&gt;] 

- Similarly, below, even when a causal relationship does exist, the statistical association is poorly estimated due to *partial* confounding through `\(U\)`.

.center[&lt;img src="./assets/partial_confounding.jpg" width="200"&gt;]

---
# Confounding

.footnote[.red[*]Ch 16 by Maria Glymour in Methods Social Epidemiology]

- Confounding is specific to variables of interest. Here, `\(U\)` confounds the `\(X \rightarrow Y\)` relationship, but does not confound the `\(Z \rightarrow Y\)` relationship.

.center[&lt;img src="./assets/iit_iv.jpg" width="200"&gt;]

- Blocking a confounder or a *sufficient set* of confounders allows for the proper estimation of causal effect.

.center[&lt;img src="./assets/blocked_confounder.jpg" width="200"&gt;]
---
# Confounding: Example



---
# Collider/Selection Bias

.footnote[.red[*]Ch 16 by Maria Glymour in Methods Social Epidemiology]

- Collider bias is "the change in association between two variables when conditioning on their common effect". 

- Most common form is selection bias, but can also occur due to stratification, M-bias, or improper covariate adjustment.

- When we condition on some common effect (or a descendent of that common effect), we open up the path that was previously blocked by the collider. 

.center[&lt;img src="./assets/selection_bias.jpg" width="200"&gt;]

---
# Collider/Selection Bias: Example
 
.center[&lt;img src="./assets/nba_players.jpg" width="600"&gt;]

???

Let's pretend in order to play in the NBA, you have to either be very fast or very tall. This is Isaiah Thomas. He's 5' 9". The average height in the NBA is 6' 7". What do we know about his athletic ability? 

We know he is fast, but we only know that because we are looking at NBA players. In the general population (that is, if we didn't condition on being in the NBA), there's no reason to believe there is a connection between height and speed.

---

# Strength of RCTs

- Note that for an ideal RCT, the biases in (1), (3), and (4) are addressed within the design of the RCT.

    - (1) large sample size and verify estimates with confidence intervals
    - (3) Temporality is built into a trial
    - (4) Exchangeability (i.e., lack of confounding) is expected in randomization
    
- Proper sampling methods and/or careful interpetation of the results can address (5).

---

### NOTES TO MYSELF

- I ignored measurement error which has a structure that can be shown in DAGs but is likely outside the scope of the class.
- Maria Glymour has a very [good numerical example of survivor bias in 16.5](http://publicifsv.sund.ku.dk/~nk/epiF14/Glymour_DAGs.pdf)



---
class: center, middle
# Thanks!

---
# Sources

- [Chapter 16 Methods in Social Epidemiology](http://publicifsv.sund.ku.dk/~nk/epiF14/Glymour_DAGs.pdf) by Maria Glymour


---
# Additional Reading
- TODO

---
class: center, middle
# Additional slides

---
# Example of M-Bias
.footnote[.red[*]Ch 16 by Maria Glymour in Methods Social Epidemiology]
.center[&lt;img src="./assets/m_bias_example.jpg" width="650"&gt;]

- Controlling for mother's diabetes status induces a correlation between low education and diabetes.

---
# Example of Survivor Bias
.footnote[.red[*]Ch 16 by Maria Glymour in Methods Social Epidemiology]
.center[&lt;img src="./assets/survivor_bias.jpg" width="600"&gt;]

---
# Other measures of causal effect

If exchangeability holds, we can express the causal effects in different ways:

1. **Absolute difference**: `\(ATE = Pr[Y=1|A=1] - Pr[Y=1|A=0]\)`

1. **Risk ratio**: `\(ATE = \frac{Pr[Y=1|A=1]}{Pr[Y=1|A=0]}\)`

1. **Odds ratio**: `\(ATE = \frac{Pr[Y=1|A=1] / Pr[Y=0|A=1]}{Pr[Y=1|A=0] / Pr[Y=1|A=0]}\)`
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('showSlide', function (slide) {setTimeout(function() {window.dispatchEvent(new Event('resize'));}, 100)});</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
