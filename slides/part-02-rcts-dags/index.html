<!DOCTYPE html>
<html>
  <head>
    <title>Evaluating causality in an ideal setting</title>
    <meta charset="utf-8">
    <meta name="author" content="Mathew Kiang, Zhe Zhang, Monica Alexander" />
    <meta name="date" content="2017-03-15" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
    <link rel="stylesheet" href="../custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Evaluating causality in an <em>ideal</em> setting
## RCTs and DAGs
### Mathew Kiang, Zhe Zhang, Monica Alexander
### March 15, 2017

---



&lt;!--
class: center, middle


.center[&lt;img src="./assets/do_you_like_dags.jpg"&gt;]
--&gt;

# Roadmap

???

So we've got some of the basic definitions down. We know what a causal effect is and we have a framework for estimating causal effects under certain assumptions. Now we're going to talk about the best case scenario for estimaing causal effects â€” RCTs. **NEXT SLIDE**

While you probably won't a lot of randomized experiments in the real world, they are useful to think about because ultimately every method for causal inference is trying to mimic an RCT. **NEXT SLIDE**

Then we'll take a bit of a detour and talk about DAGs. I think they're very useful; however, a lot of people don't. Regardless, you should know about them because you'll likely come across them in the causal inference literature. And hopefully I can convince you that they provide a useful way of linking your causal question to your data and regressions. **NEXT SLIDE**

Finally, we'll combine RCTs and DAGs to show how bias can affect estimates when we don't have an RCT (and why RCTs are so powerful). Then next lecture, Zhe will talk more about the math behind these different biases.

--

### Randomized Control Trials

- Thinking about causality in the ideal setting
--

### Causal Directed Acyclic Graphs (DAGs)

- Useful ways of encoding our beliefs and assumptions

- Helpful for thinking about ways our assumptions can be wrong
--

### Use DAGs to show how RCTs avoid bias

- More about these biases next lecture


---
# Randomized Control Trials

.pull-right[&lt;img src="./assets/james_lind.jpg" width="250"&gt;]

???

Again, we'll start off with a bit of motivation. This is James Lind. He was a Scottish physician in the Royal Navy **NEXT SLIDE**

He conducted the first recorded RCT on scurvy, which is a disease from Vitamin C deficiency and causes pretty bad symptoms. It's a disease of the connective tissue because Vitamin C is required to create connective tissue, all sorts of bad things happen. Severe bleeding, gum disease, inability to heal from superficial wounds, etc. People eventually get personality changes and then die of infection or bleeding. That said, it's very easy to treat. Just some Vitamin C and improvement occurs in days and full recovery in about a month. Was very common back when there were long sea voyages and they were unable to keep fresh fruit available. **NEXT SLIDE**

So Lind was on a ship and he found 12 sailrs with scurvey. Randomly split them up into 6 groups and gave each group a different treatment in addition to their regular diet. **NEXT SLIDE**

As he suspected, only the group that had citrus fruits showed substantial improvement with one returning back to work and one on the mend (they ran out of fruit before he could completely heal).

--
.footnote[Wikipedia]

- First recorded RCT was done in 1747 by James Lind

--

- Scurvy is a terrible disease caused by Vitamin C deficiency and was a huge issue during long sea voyages

--

- Lind took 12 sailors with scurvy and split them into six groups of two:

    - Groups were assigned: (1) 1 qt cider, (2) 25 drops of vitriol, (3) 6 spoonfuls of vinegar, (4) 1/2 pt of sea water, (5) garlic, mustard, and barley water, (6) 2 oranges and 1 lemon

--

- Only Group 6 (citrus fruit) showed substantial improvement

---
# Randomized Control Trials
.footnote[Hernan Epi 201]

### What's the big deal about RCTs?

???

There are a few things worth nothing about Lind's RCT. **NEXT SLIDE**

First, he showed a strong causal effect with a tiny sample. An n=12 study would literally never get published today, but nonetheless, it's hard to argue with a strong causal effect even in a small RCT. **NEXT SLIDE**

Second, even though Lind was wildly wrong about **why** the RCT worked, he was right about the causal effect of citric acid on preventing and curing scurvy. Back then, medicine didn't reocgnize the importance of vitamins and so Lind believed that survey or all the bleeding was a result of the body beging to putrefy and acid reduced the process.

--

- Even with a very small sample size, Lind was able to show convincing causal effect

--

- RCTs allow us to gain knowledge about causal effects **without knowing the mechanism** 

    - Lind believed acid stopped putrefaction. 
    - Medicine had not yet understood importance of vitamins.
    
---
# Randomized Control Trials
### But RCTs aren't perfect


- Not every question can be answered with an RCT (feasibility or ethical concerns)

???

However, it is worth noting that you can't always do an RCT. You've never read an RCT on the causal effects of smoking and lung cancer in humans. It would be wildly unethical to force some humans to smoke and prevent others from doing so. The famous example people like to use is that you've never read an RCT on the effectiveness of parachutes on preventing death from jumping out of a plane. We know they work. We have observational studies to indicate it works. We have strong priors. Etc. **NEXT SLIDE**

It's also important to note that RCTs are often very costly and have a smaller, more select sample size. While the *internal* validity is high, the *external* validity may not be. In other words, I know the causal effect is true for people *inside of my study*. But I cannot be certain that it is true for everybody outside of my study as well.


--

- Smaller sample size often means limited generalizability. The *internal validity* of an RCT is very high, but the *external validity* is often not.

---
# RCT Definitions

.footnote[Hernan Epi 201]

- **Randomized**: The intervention is assigned randomly


--

- **Control**: There is (at least) one well-defined comparison group

--

- **Experiment**: Investigators (directly) control the intervention

- **Trial**: An experiment where the goal is to study effect of some medical intervention on humans

    - Trials have ethical requirements that experiments may not (e.g., equipoise)

--
&lt;br&gt;
### In an ideal RCT, there is no loss to follow-up, perfect compliance, only one version of treatment, double (or triple) blind assignment
---
# Randomized Control Trials

- Recall from last lecture, because of the *Fundamental Problem of Causal Inference*, we can never observe both counterfactuals.

--

- Instead, we ask: under what conditions do the data we have properly estimate the data we want? `$$Pr(Y^{a=1})-Pr(Y^{a=0})=Pr(Y|A=1)-Pr(Y|A=0)$$`
--

- Again, recall our assumptions for the Rubin causal model:
--

    - How do we ensure our groups *exchangeable*?
    - That we have *positivity*?
    - And that we have *stability*?

--
### .center[These are all met in an ideal RCT]

---
# Randomized Control Trials

.footnote[Hernan Epi 201]

- Suppose we have a large (nearly infinite) population of perfect compliers

--

- We divide them into two groups: (1) Group Heads and (2) Group Tails 

--

- We assigned the groups by flipping a (fair) coin

--

- Now, we decide to treat Group Heads and not treat Group Tails 

    - The risk of death in Group Heads is: `\(Pr[Y=1|A=1] = .25\)`

--

- Pretend that instead of doing this, we treated Group Tails and did not treat Group Heads. What would the expected risk of death in Group Tails be?

--

    - Still `\(.25\)`

---
class: center, middle
# Exchangeability is a consequence of randomization

--

## Unfortunately, we (usually) won't get to randomize

---
# Formal definition of exchangeability
.footnote[Hernan Epi 201]

The counterfactual outcome `\(Y^a\)` is independent of the actual treatment `\(A\)`.
`$$Y^a \perp A \text{ for all } a$$`

--

- Exchangeability is "another causal concept that cannot be represented by associational (statistical) language"

--

### NOTE!

Exchanagebility is not the same as independence. That is, `$$Y^a \perp A \neq Y \perp A$$`

---
# Aside: Measures of causal effect

If exchangeability holds, we can express the causal effects in different ways:

--

1. **Absolute difference**: `\(ATE = Pr[Y=1|A=1] - Pr[Y=1|A=0]\)`

--

1. **Risk ratio**: `\(ATE = \frac{Pr[Y=1|A=1]}{Pr[Y=1|A=0]}\)`

--

1. **Odds ratio**: `\(ATE = \frac{Pr[Y=1|A=1] / Pr[Y=0|A=1]}{Pr[Y=1|A=0] / Pr[Y=1|A=0]}\)`

---
class: center, middle, inverse
# The Life-changing Magic of DAGs

---
# Causal Directed Acyclic Graphs (DAGs)

.center[&lt;img src="./assets/basic_dag.jpg" width="200"&gt;]
.footnote[.red[*]Ch 16 by Maria Glymour in Methods Social Epidemiology]

- Causal DAGs are an easy tool for us to express statistical associations implied by our assumptions in their causal structure. 

--

- They also allow us to do the reverse. Given a set of associations in our data, diagram the causal structures that could result in those associations.

--

- Causal DAGs provide a mathematical link between statistics and causality.


---
# Causal Directed Acyclic Graphs (DAGs)

.center[&lt;img src="./assets/basic_dag.jpg" width="200"&gt;]
.footnote[.red[*]Ch 16 by Maria Glymour in Methods Social Epidemiology]

- Graphically, the goal is to estimate magnitude and direction arrow. Statistically, we can only estimate the magnitude

--

- The edge itself can exist for at least 5 reasons:
    1. Just random luck
    1. `\(X\)` really does cause `\(Y\)` (causal)
    1. `\(Y\)` actually causes `\(X\)` (reverse causation)
    1. `\(X\)` and `\(Y\)` share a common cause (confounding)
    1. `\(X\)` and `\(Y\)` have a common effect that we are conditioning on (selection)

--

- Our job is to figure out which of these reasons is most consistent with the data and rule out all other explanations
    
---
# Rules for drawing a DAG

.center[&lt;img src="./assets/example_dag.jpg" width="200"&gt;]
.footnote[.red[*]Ch 16 by Maria Glymour in Methods Social Epidemiology]

- Time flows left to right (thus arrows always point right)

--

- An arrow implies our belief that something is *causal*. Conversely, the lack of an arrow implies our belief that things are *not* causal

--

- Everything we are concerned with exists in the DAG

---
# Reading DAGs: Causal assumptions

.center[&lt;img src="./assets/example_dag.jpg" width="200"&gt;]
.footnote[.red[*]Ch 16 by Maria Glymour in Methods Social Epidemiology]

- `\(X\)` and `\(U\)` are direct causes of `\(Y\)`
--

- `\(Y\)` is a direct cause of `\(Z\)`
--

- `\(X\)` and `\(U\)` are *not* direct causes of `\(Z\)`, but *indirect* causes through `\(Y\)`
--

- `\(X\)` and `\(U\)` are not causes of each other
--

- No two variables share a common cause

---
# Reading DAGs: Statistical implications

.center[&lt;img src="./assets/example_dag.jpg" width="200"&gt;]
.footnote[.red[*]Ch 16 by Maria Glymour in Methods Social Epidemiology]

- An edge implies statistical dependence
--

- No statistical dependence if there is a *collider*. Two arrow heads into one variable (e.g., `\(U \rightarrow Y \leftarrow X\)`)
--

- No statistical dependence if we *condition* or *block* a variable (denoted with a box around that variable) or a *descendent* of that variable
--

- There is statistical dependence if we block a collider or a descendent of that collider

---
# Reading DAGs: Statistical implications
.footnote[.red[*]Ch 16 by Maria Glymour in Methods Social Epidemiology]

.center[&lt;img src="./assets/example_dag.jpg" width="200"&gt;]

- `\(X\)` and `\(Y\)` are statistically dependent
- `\(U\)` and `\(Y\)` are statistically dependent
- `\(Y\)` and `\(Z\)` are statistically dependent
--

- `\(X\)` and `\(Z\)` are statistically dependent (i.e., `\(X \rightarrow Y \rightarrow Z\)`)
- `\(U\)` and `\(Z\)` are statistically dependent (i.e., `\(U \rightarrow Y \rightarrow Z\)`)

---
# Reading DAGs: Statistical implications
.footnote[.red[*]Ch 16 by Maria Glymour in Methods Social Epidemiology]

.center[&lt;img src="./assets/example_dag.jpg" width="200"&gt;]

- `\(X\)` and `\(U\)` are statistically independent (blocked by a collider `\(Y\)`)

--
- `\(X\)` and `\(U\)` are statistically dependent, conditional on `\(Y\)` (conditioning on a collider, unblocks that path)

--
- `\(X\)` and `\(U\)` are statistically dependent, conditional on `\(Z\)` (conditioning on a collider's descendent, unblocks that path)

--
- `\(X\)` and `\(Z\)` are statistically independent, conditional on `\(Y\)` (conditioning on `\(Y\)` blocks the `\(X \rightarrow Y \rightarrow Z\)` path)

--
- Similar to above, `\(U\)` and `\(Z\)` are statistically independent, conditional on `\(Y\)`.

---
# DAGs Assumptions
.footnote[.red[*]Ch 16 by Maria Glymour in Methods Social Epidemiology]

.center[&lt;img src="./assets/example_dag.jpg" width="200"&gt;]

1. **Causal Markov Assumption.** Any variable `\(X\)` is independent of any other variable `\(Y\)` conditional on on the direct causes of `\(X\)` (unless `\(Y\)` is an effect of `\(X\)`).
1. **Faithfulness.** Positive and negative causal effects never *perfectly* offset. 
1. **Neglible randomness.** Statistical associations (or lack of such associations) are never due to random change (i.e., large sample size assumption).

---
# RCTs and DAGs

- Earlier, I said the edge in a DAG can exist for five reasons:
    1. Just random luck
    1. `\(X\)` really does cause `\(Y\)` (causal)
    1. `\(Y\)` actually causes `\(X\)` (reverse causation)
    1. `\(X\)` and `\(Y\)` share a common cause (confounding)
    1. `\(X\)` and `\(Y\)` have a common effect that we are conditioning on (selection)

--

### RCTs address most of these

- Large sample size allows for checking (1) via confidence intervals

- Temporal ordering excludes (2)

- Randomization excludes (3)


---
class: center, middle
# Thanks!

---
# Sources

- [Chapter 16 Methods in Social Epidemiology](http://publicifsv.sund.ku.dk/~nk/epiF14/Glymour_DAGs.pdf) by Maria Glymour


---
# Additional Reading
- TODO
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('showSlide', function (slide) {setTimeout(function() {window.dispatchEvent(new Event('resize'));}, 100)});</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
